{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:07:53.382909Z","iopub.execute_input":"2025-06-12T07:07:53.383609Z","iopub.status.idle":"2025-06-12T07:07:53.651294Z","shell.execute_reply.started":"2025-06-12T07:07:53.383586Z","shell.execute_reply":"2025-06-12T07:07:53.650730Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Install required packages\n!pip install -q transformers accelerate bitsandbytes langchain langchain_community sentencepiece einops\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:07:55.057829Z","iopub.execute_input":"2025-06-12T07:07:55.058454Z","iopub.status.idle":"2025-06-12T07:09:20.576906Z","shell.execute_reply.started":"2025-06-12T07:07:55.058431Z","shell.execute_reply":"2025-06-12T07:09:20.576147Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.llms import HuggingFacePipeline\nimport torch\nimport re\nimport json\n\n# Verify environment\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(f\"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:09:20.578524Z","iopub.execute_input":"2025-06-12T07:09:20.578849Z","iopub.status.idle":"2025-06-12T07:09:46.057894Z","shell.execute_reply.started":"2025-06-12T07:09:20.578822Z","shell.execute_reply":"2025-06-12T07:09:46.057301Z"}},"outputs":[{"name":"stderr","text":"2025-06-12 07:09:33.262210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749712173.460154      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749712173.515141      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nGPU available: True\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3:\nmodel_name = \"Qwen/Qwen3-4B\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    pad_token='<|endoftext|>',\n    padding_side='left'\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\n# Create separate pipelines for different tasks\ninfo_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=256,\n    temperature=0.3,  \n    top_p=0.9,\n    repetition_penalty=1.3,\n    do_sample=True,\n    eos_token_id=tokenizer.eos_token_id\n)\n\nquestion_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=1024,  \n    temperature=1,    \n    top_p=0.95,\n    repetition_penalty=1.3,\n    do_sample=True,\n    eos_token_id=tokenizer.eos_token_id\n)\n\ninfo_llm = HuggingFacePipeline(pipeline=info_pipeline)\nquestion_llm = HuggingFacePipeline(pipeline=question_pipeline)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:09:46.058532Z","iopub.execute_input":"2025-06-12T07:09:46.059049Z","iopub.status.idle":"2025-06-12T07:11:11.870107Z","shell.execute_reply.started":"2025-06-12T07:09:46.059026Z","shell.execute_reply":"2025-06-12T07:11:11.869291Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce5394e1828a4c73909f365487879cbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d989d105c52b4dd481b6ff840eecf558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166432edfb904e21b910d640f39492ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f443491178246b3b8f052f80190676d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25c0e1f51aa45cf93406da8e064da4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/32.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0c4844b6d94db9a439503f3114ea98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a8e25522544bb8af52a11ada7d5365"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa6d23532424abcbb8ffb09697236e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a02a554dfadf40bfbfbaf4b6d5010c3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f3378bf40d4926a72d62e0857ef1df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca3277a63934d7d871590688d3c9163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60a5e96f3cb445f0807e030d674fbdeb"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\nDevice set to use cuda:0\n/tmp/ipykernel_35/2102704115.py:41: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  info_llm = HuggingFacePipeline(pipeline=info_pipeline)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 4:\n# Updated prompt templates\nsystem_prompt = \"\"\"<|im_start|>system\nYou are TalentBot, an AI hiring assistant for TalentScout recruitment agency. Your job is asking technical question to the candidate and collect their answers.\nYour objectives are:\n1. Professionally collect candidate information.\n2. Generate high-quality, meaningful and complete technical questions.\n3. Maintain context and a coherent conversational flow with candidate.\n4. Never deviate from your purpose of assisting with candidate screening.\n5. Carefully resolve candidate's query during screening test. First figure out what is the candidate query and context along that.\n6. When evaluating or presenting code snippets, ensure the code is complete, syntactically correct, and logically sound.\n7. Promptly and accurately address any clarifications or queries the candidate raises about the assessment itself, first understanding their question fully before responding.\nCurrent conversation stage: {stage}\n{history}<|im_end|>\n\"\"\"\n\ninfo_gathering_template = system_prompt + \"\"\"<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\ntech_question_template = \"\"\"<|im_start|>system\nYou are TalentBot, the technical interviewer for a candidate. The candidate is applying for the position of {desired_position} and has {years_experience} years of experience. The candidate's technology stack includes: {tech_stack}.\n\nYour task is to generate exactly 5 technical interview questions tailored to this candidate. Use the information given to ensure the questions are relevant and appropriately challenging. Follow these guidelines:\n\n- Ask exactly five questions, each on a separate line prefixed with its number (1. to 5.).\n- Tailor each question specifically to the technologies listed and the desired position.\n- Match the difficulty level to the candidate's experience ({years_experience} years).\n- Begin with more fundamental concepts and progressively increase in complexity.\n- Include a variety of question types.\n- Ensure each question is standalone, actionable, and non-overlapping in scope.\n- Avoid questions about topics not included in the candidate's tech stack.\n- Use clear and concise professional language appropriate for an interview.\n- Do NOT provide any answers or hints to the questions — only ask the questions.\n- If the candidate asks for clarification on a question, clarify the question without giving away any answer.\n- Ensure each question is specific and actionable, not overly broad or vague.\n- Focus on problem-solving approach, code implementation details, and understanding of the technologies.\n- Include questions that evaluate both theoretical understanding and hands-on skills.\n- Ensure no two questions are redundant or too similar.\n- Check that all five questions are included before finalizing. Ensure that they are valid and complete.\n- Make sure questions are clear and concise\n- Verify completeness and clarity of all five questions before delivering.\n<|im_end|>\n<|im_start|>assistant\n1.\n\"\"\"\n\nclosing_template = system_prompt + \"\"\"<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\n# Create prompt templates\ninfo_gathering_prompt = PromptTemplate(\n    input_variables=[\"stage\", \"history\", \"input\"],\n    template=info_gathering_template\n)\n\ntech_question_prompt = PromptTemplate(\n    input_variables=[\"tech_stack\", \"years_experience\", \"desired_position\"],\n    template=tech_question_template\n)\n\nclosing_prompt = PromptTemplate(\n    input_variables=[\"stage\", \"history\", \"input\"],\n    template=closing_template\n)\n\n# Create chains\ninfo_gathering_chain = LLMChain(\n    llm=info_llm,\n    prompt=info_gathering_prompt,\n    verbose=False\n)\n\ntech_question_chain = LLMChain(\n    llm=question_llm,\n    prompt=tech_question_prompt,\n    verbose=False\n)\n\nclosing_chain = LLMChain(\n    llm=info_llm,\n    prompt=closing_prompt,\n    verbose=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:11:11.871863Z","iopub.execute_input":"2025-06-12T07:11:11.872081Z","iopub.status.idle":"2025-06-12T07:11:11.879552Z","shell.execute_reply.started":"2025-06-12T07:11:11.872065Z","shell.execute_reply":"2025-06-12T07:11:11.878765Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/1514396138.py:71: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  info_gathering_chain = LLMChain(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# New chains for query handling\nrelevance_prompt = PromptTemplate(\n    input_variables=[\"current_question\", \"query_text\"],\n    template=\"\"\"<|im_start|>system\nYou are an expert interviewer assistant with a focus on evaluating the relevance of candidate queries in a technical interview context. For each pair of inputs, perform two checks:\n\nRelevance: Determine whether the Candidate Query is directly related to the Current Question.\n\nQuestion Quality: Assess whether the Current Question is well-formed—meaning it is correct, complete, and answerable as stated.\n\nRespond with exactly one of the following, with no additional text:\n\n- yes and question is correct\n\n- yes and question is incorrect\n\n- no\n\nCurrent Question: {current_question}\nCandidate Query: {query_text}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\n)\n\nrevision_prompt = PromptTemplate(\n    input_variables=[\"current_question\", \"query_text\"],\n    template=\"\"\"\n    <|im_start|>system\nYou are a senior technical interviewer and expert question writer. A candidate has requested clarification on the following interview question:\n{current_question}\n\nThey have raised this specific query for additional context or detail:\n{query_text}\n\nYour task is to deliver a revised version of the original question that:\n1. Preserves the original technical objectives and scope.\n2. Maintains the intended difficulty and challenge level.\n3. Enhances clarity, precision, and completeness.\n4. Employs a professional, engaging tone suitable for an interview setting.\n\nGuidelines:\n- If the candidate’s query highlights missing context, code examples, or parameters, integrate them directly into the revised question.\n- Do not include any commentary, explanation, or notes—return only the updated question.\n- Ensure the question remains actionable, unambiguous, and aligned with its original intent.\n<|im_end|>\n<|im_start|>assistant\n\n\"\"\"\n)\n\n# Create query handling chains\nrelevance_chain = LLMChain(\n    llm=info_llm,\n    prompt=relevance_prompt,\n    verbose=False\n)\n\nrevision_chain = LLMChain(\n    llm=question_llm,\n    prompt=revision_prompt,\n    verbose=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:11:11.880387Z","iopub.execute_input":"2025-06-12T07:11:11.880652Z","iopub.status.idle":"2025-06-12T07:11:12.735465Z","shell.execute_reply.started":"2025-06-12T07:11:11.880630Z","shell.execute_reply":"2025-06-12T07:11:12.734590Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 5:\nclass HiringState:\n    FIELDS = [\n        \"full_name\",\n        \"email\",\n        \"phone\",\n        \"years_experience\",\n        \"desired_position\",\n        \"current_location\"\n    ]\n    FIELD_DISPLAY = {\n        \"full_name\": \"full name\",\n        \"email\": \"email address\",\n        \"phone\": \"phone number\",\n        \"years_experience\": \"years of professional experience\",\n        \"desired_position\": \"desired position\",\n        \"current_location\": \"current location\",\n        \"tech_stack\": \"tech stack (comma-separated list of technologies)\"\n    }\n    FIELD_PROMPTS = {\n        \"full_name\": \"What is your full name?\",\n        \"email\": \"What is your email address?\",\n        \"phone\": \"What is your phone number?\",\n        \"years_experience\": \"How many years of professional experience do you have?\",\n        \"desired_position\": \"What is your desired position?\",\n        \"current_location\": \"What is your current location?\",\n        \"tech_stack\": \"Please list your technical skills (comma-separated):\"\n    }\n    FIELD_VALIDATORS = {\n        \"email\": lambda x: re.match(r\"[^@]+@[^@]+\\.[^@]+\", x) is not None,\n        \"phone\": lambda x: re.match(r\"^\\+?[0-9\\s\\-\\(\\)]{7,}$\", x) is not None,\n        \"years_experience\": lambda x: re.match(r\"^\\d+$\", x) is not None\n    }\n    FIELD_ERRORS = {\n        \"email\": \"Please enter a valid email address (e.g., name@example.com).\",\n        \"phone\": \"Please enter a valid phone number (e.g., +1 123-456-7890).\",\n        \"years_experience\": \"Please enter a valid number of years (e.g., 5).\"\n    }\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.stage = \"greeting\"\n        self.current_field_idx = 0\n        self.candidate_data = {field: None for field in self.FIELDS}\n        # Ensure tech_stack key exists in candidate_data\n        self.candidate_data[\"tech_stack\"] = None\n        self.tech_questions = []\n        self.current_question_idx = 0\n        self.memory = ConversationBufferMemory()\n        self.conversation_log = []\n\n    def get_current_field(self):\n        if self.current_field_idx < len(self.FIELDS):\n            return self.FIELDS[self.current_field_idx]\n        return None\n\n    def record_response(self, field, value):\n        if field in self.FIELDS:\n            self.candidate_data[field] = value\n            self.log_interaction(f\"User provided {field}: {value}\")\n\n    def next_field(self):\n        if self.current_field_idx < len(self.FIELDS) - 1:\n            self.current_field_idx += 1\n            return True\n        return False\n\n    def log_interaction(self, message):\n        self.conversation_log.append(message)\n\n    def to_dict(self):\n        return {\n            \"stage\": self.stage,\n            \"current_field\": self.get_current_field(),\n            \"candidate_data\": self.candidate_data,\n            \"tech_questions\": self.tech_questions,\n            \"current_question_idx\": self.current_question_idx\n        }\n\nsentiment_pipeline = pipeline(\n    \"sentiment-analysis\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n    truncation=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:11:12.736296Z","iopub.execute_input":"2025-06-12T07:11:12.736586Z","iopub.status.idle":"2025-06-12T07:11:18.934589Z","shell.execute_reply.started":"2025-06-12T07:11:12.736561Z","shell.execute_reply":"2025-06-12T07:11:18.933943Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d80d0b4636746f3ab4fb75e9e34f008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1740cffa7e194335bdb543090c2df12d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85299c79de3a4ab6aa0910b49bee8311"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e1cd16ee114ec39a9a04547df3a297"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# cell 6:\ndef clean_response(response):\n    \"\"\"Clean and format the model response\"\"\"\n    # Remove special tokens\n    response = re.sub(r'<\\|im_start\\|>.*?<\\|im_end\\|>', '', response, flags=re.DOTALL)\n    response = re.sub(r'<\\|.*?\\|>', '', response, flags=re.DOTALL)\n    # Remove thinking tags and content\n    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n    if \"assistant\" in response.lower():\n        parts = re.split(r'assistant', response, flags=re.IGNORECASE)\n        if len(parts) > 1:\n            response = parts[-1].strip()\n    response = re.sub(r'^\\d+[\\.\\)\\s]*', '', response).strip()\n    response = re.sub(r'\\n+', '\\n', response).strip()\n    return response\n\ndef extract_questions(response):\n    \"\"\"Extract generated questions from model response\"\"\"\n    questions = []\n    lines = response.split('\\n')\n    for line in lines:\n        match = re.match(r'(\\d+)\\.?\\s*(.*)', line.strip())\n        if match:\n            question_num = int(match.group(1))\n            question_text = match.group(2).strip()\n            if len(question_text) > 20 and '?' in question_text:\n                questions.append(f\"{question_num}. {question_text}\")\n    if len(questions) < 5:\n        pattern = r'\\d+\\.\\s*([^\\n?]+\\??)'\n        alt_questions = re.findall(pattern, response)\n        questions = [f\"{i+1}. {q.strip()}\" for i, q in enumerate(alt_questions[:5])]\n    return questions[:5]\n\ndef format_tech_stack(tech_input):\n    return [tech.strip() for tech in tech_input.split(',') if tech.strip()]\n\ndef analyze_sentiment(text):\n    if len(text) < 3:\n        return {\"label\": \"NEUTRAL\", \"score\": 0.0}\n    try:\n        result = sentiment_pipeline(text, truncation=True)[0]\n        return {\"label\": result['label'], \"score\": float(result['score'])}\n    except:\n        return {\"label\": \"ERROR\", \"score\": 0.0}\n\ndef print_state(state):\n    \"\"\"Debug function to print current state\"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Current Stage: {state.stage}\")\n    print(f\"Current Field: {state.get_current_field()}\")\n    print(f\"Questions: {len(state.tech_questions)}\")\n    print(f\"Data: {json.dumps(state.candidate_data, indent=2)}\")\n    print(\"=\"*50 + \"\\n\")\n\ndef generate_tech_questions(state):\n    \"\"\"Generate technical questions with better prompting and validation\"\"\"\n    required_fields = ['tech_stack', 'years_experience', 'desired_position']\n    if any(state.candidate_data[field] is None for field in required_fields):\n        state.stage = \"closing\"\n        return \"We don't have enough information to generate questions. Thank you for your time.\", state\n\n    response = tech_question_chain.invoke({\n        \"tech_stack\": \", \".join(state.candidate_data[\"tech_stack\"]),\n        \"years_experience\": state.candidate_data[\"years_experience\"],\n        \"desired_position\": state.candidate_data[\"desired_position\"]\n    })['text']\n\n    cleaned_response = clean_response(response)\n    state.tech_questions = extract_questions(cleaned_response)\n\n    if not state.tech_questions:\n        position = state.candidate_data[\"desired_position\"] or \"this position\"\n        tech = state.candidate_data[\"tech_stack\"][0] if state.candidate_data[\"tech_stack\"] else \"your primary technology\"\n        state.tech_questions = [\n            \"1. What experience do you have with \" + tech + \"?\",\n            \"2. Describe a challenging project you've worked on using \" + tech + \".\",\n            \"3. How would you debug a performance issue in a \" + tech + \" application?\",\n            \"4. What best practices do you follow when working with \" + tech + \"?\",\n            \"5. How does your experience align with the requirements for \" + position + \"?\"\n        ]\n\n    state.log_interaction(f\"Generated {len(state.tech_questions)} technical questions\")\n    return None, state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:11:18.935501Z","iopub.execute_input":"2025-06-12T07:11:18.935781Z","iopub.status.idle":"2025-06-12T07:11:18.948119Z","shell.execute_reply.started":"2025-06-12T07:11:18.935754Z","shell.execute_reply":"2025-06-12T07:11:18.947527Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def handle_conversation(user_input, state):\n    # Exit condition\n    if user_input.lower() in ['exit', 'quit', 'stop', 'end']:\n        closing_response = \"Thank you for your time. We'll be in touch soon!\"\n        state.memory.save_context({\"input\": user_input}, {\"output\": closing_response})\n        state.log_interaction(f\"Assistant: {closing_response}\")\n        return closing_response, state\n\n    if user_input:\n        state.log_interaction(f\"User: {user_input}\")\n        if state.stage not in [\"greeting\", \"awaiting_start\", \"info_gathering\"]:\n            sentiment = analyze_sentiment(user_input)\n            state.log_interaction(f\"Sentiment: {sentiment['label']} ({sentiment['score']:.2f})\")\n\n    state.memory.save_context({\"input\": user_input}, {\"output\": \"\"})\n\n    # Handle query requests during technical interview\n    if state.stage == \"technical_interview\" and user_input.lower().startswith(\"query:\"):\n        query_text = user_input[len(\"query:\"):].strip()\n        current_question = state.tech_questions[state.current_question_idx]\n        \n        # Check relevance of query\n        relevance_response = relevance_chain.invoke({\n            \"current_question\": current_question,\n            \"query_text\": query_text\n        })['text'].lower().strip()\n        \n        if \"yes\" in relevance_response:\n            # Generate revised question\n            revised_question = revision_chain.invoke({\n                \"current_question\": current_question,\n                \"query_text\": query_text\n            })['text'].strip()\n            \n            # Clean and store revised question\n            revised_question = clean_response(revised_question)\n            state.tech_questions[state.current_question_idx] = revised_question\n            state.log_interaction(f\"Revised Question: {revised_question}\")\n            \n            response = f\"Thank you for your query. Here's the revised question:\\n\\n{revised_question}\"\n        else:\n            response = \"Your query doesn't appear relevant to the current question. Please answer the original question.\"\n        \n        state.memory.save_context({\"input\": user_input}, {\"output\": response})\n        state.log_interaction(f\"Assistant: {response}\")\n        return response, state\n\n    # Existing conversation flow\n    if state.stage == \"greeting\":\n        greeting = \"Hello! I'm TalentBot from TalentScout. I'll guide you through our initial screening process. \"\\\n                   \"If you're ready to begin, please type 'Start'.\\n\\n\"\\\n                   \"You can type 'exit' at any time to end the conversation.\\n\"\\\n                   \"During technical questions, you can request clarification by typing 'query: your question'.\"\n        state.stage = \"awaiting_start\"\n        state.log_interaction(\"System: Initial greeting\")\n        return greeting, state\n\n    elif state.stage == \"awaiting_start\":\n        if user_input.lower() == \"start\":\n            state.stage = \"info_gathering\"\n            state.log_interaction(\"User started the process\")\n            first_field = state.get_current_field()\n            return state.FIELD_PROMPTS[first_field], state\n        else:\n            return \"Please type 'Start' when you're ready to begin the screening process.\", state\n\n    elif state.stage == \"info_gathering\":\n        current_field = state.get_current_field()\n        if current_field in state.FIELD_VALIDATORS:\n            validator = state.FIELD_VALIDATORS[current_field]\n            if not validator(user_input):\n                return state.FIELD_ERRORS[current_field], state\n        state.record_response(current_field, user_input)\n        if state.next_field():\n            next_field = state.get_current_field()\n            return state.FIELD_PROMPTS[next_field], state\n        else:\n            state.stage = \"tech_stack_collection\"\n            state.log_interaction(\"System: Collecting tech stack\")\n            return state.FIELD_PROMPTS[\"tech_stack\"], state\n\n    elif state.stage == \"tech_stack_collection\":\n        tech_stack = format_tech_stack(user_input)\n        state.candidate_data[\"tech_stack\"] = tech_stack\n        state.log_interaction(f\"Tech stack: {', '.join(tech_stack)}\")\n        state.stage = \"technical_interview\"\n        msg, state = generate_tech_questions(state)\n        if msg:\n            return msg, state\n        if state.tech_questions:\n            first_question = state.tech_questions[0].split('.', 1)[1].strip()\n            response = f\"Thank you! Let's begin the technical assessment. You'll be asked {len(state.tech_questions)} questions.\\n\\n\"\\\n                       f\"Question 1: {first_question}\\n\\n\"\\\n                       \"If you need clarification on any question, type 'query: your question'.\"\n        else:\n            state.stage = \"closing\"\n            response = \"We've completed the initial screening. Thank you for your time!\"\n        state.memory.save_context({\"input\": user_input}, {\"output\": response})\n        state.log_interaction(f\"Assistant: {response}\")\n        return response, state\n\n    elif state.stage == \"technical_interview\":\n        if state.current_question_idx < len(state.tech_questions):\n            question = state.tech_questions[state.current_question_idx]\n            state.log_interaction(f\"Question {state.current_question_idx+1}: {question}\")\n            \n            # Only log as answer if it's not a query request\n            if not user_input.lower().startswith(\"query:\"):\n                state.log_interaction(f\"Answer: {user_input[:200]}...\")\n                state.current_question_idx += 1\n            \n            if state.current_question_idx < len(state.tech_questions):\n                next_q = state.tech_questions[state.current_question_idx].split('.', 1)[1].strip()\n                response = f\"Question {state.current_question_idx + 1}: {next_q}\"\n            else:\n                state.stage = \"closing\"\n                state.log_interaction(\"Completed technical assessment\")\n                response = \"Thank you for completing the assessment! Our team will review your answers and contact you soon.\"\n            \n            state.memory.save_context({\"input\": user_input}, {\"output\": response})\n            state.log_interaction(f\"Assistant: {response}\")\n            return response, state\n\n    if state.stage == \"closing\":\n        response = \"Thank you again! Our team will review your application shortly.\"\n        state.memory.save_context({\"input\": user_input}, {\"output\": response})\n        state.log_interaction(f\"Assistant: {response}\")\n        return response, state\n\n    response = \"I'm here to assist with your job application. Could you please rephrase that?\"\n    state.memory.save_context({\"input\": user_input}, {\"output\": response})\n    state.log_interaction(f\"Assistant: {response}\")\n    return response, state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:11:18.948793Z","iopub.execute_input":"2025-06-12T07:11:18.949001Z","iopub.status.idle":"2025-06-12T07:11:18.966968Z","shell.execute_reply.started":"2025-06-12T07:11:18.948984Z","shell.execute_reply":"2025-06-12T07:11:18.966324Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 9:\ndef run_conversation():\n    state = HiringState()\n    print(\"\\n\" + \"=\"*50)\n    print(\"TalentScout Hiring Assistant\")\n    print(\"Type 'exit' at any time to end the conversation\")\n    print(\"=\"*50 + \"\\n\")\n\n    response, state = handle_conversation(\"\", state)\n    print(f\"Assistant: {response}\")\n\n    while True:\n        user_input = input(\"Candidate: \")\n        if user_input.lower() in ['exit', 'quit', 'stop', 'end']:\n            print(\"\\nAssistant: Thank you for your time. The conversation has ended.\")\n            break\n        response, state = handle_conversation(user_input, state)\n        print(f\"\\nAssistant: {response}\")\n        if state.stage == \"closing\" and \"thank\" in response.lower():\n            break\n\n    print(\"\\n===== Conversation Summary =====\")\n    print(f\"Total Interactions: {len(state.conversation_log)}\")\n\n    sentiment_counts = {\"POSITIVE\": 0, \"NEGATIVE\": 0, \"NEUTRAL\": 0}\n    sentiment_scores = []\n    for log in state.conversation_log:\n        if log.startswith(\"Sentiment:\"):\n            parts = log.split(':')\n            if len(parts) > 1:\n                sentiment = parts[1].strip().split()[0]\n                if sentiment in sentiment_counts:\n                    sentiment_counts[sentiment] += 1\n                if '(' in parts[1]:\n                    score_str = parts[1].split('(')[1].split(')')[0]\n                    try:\n                        sentiment_scores.append(float(score_str))\n                    except:\n                        pass\n\n    if sentiment_scores:\n        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n        print(f\"\\nSentiment Analysis:\")\n        print(f\"- Positive Responses: {sentiment_counts['POSITIVE']}\")\n        print(f\"- Negative Responses: {sentiment_counts['NEGATIVE']}\")\n        print(f\"- Neutral Responses: {sentiment_counts['NEUTRAL']}\")\n        print(f\"- Average Sentiment Score: {avg_sentiment:.2f}\")\n        print(f\"- Overall Tone: {'Positive' if avg_sentiment > 0.6 else 'Negative' if avg_sentiment < 0.4 else 'Neutral'}\")\n\n    \n\n# Start conversation\nrun_conversation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:00:48.853751Z","iopub.execute_input":"2025-06-11T17:00:48.854013Z","iopub.status.idle":"2025-06-11T17:11:17.645688Z","shell.execute_reply.started":"2025-06-11T17:00:48.853995Z","shell.execute_reply":"2025-06-11T17:11:17.644988Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTalentScout Hiring Assistant\nType 'exit' at any time to end the conversation\n==================================================\n\nAssistant: Hello! I'm TalentBot from TalentScout. I'll guide you through our initial screening process. If you're ready to begin, please type 'Start'.\n\nYou can type 'exit' at any time to end the conversation.\nDuring technical questions, you can request clarification by typing 'query: your question'.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  Start\n"},{"name":"stdout","text":"\nAssistant: What is your full name?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  alpha\n"},{"name":"stdout","text":"\nAssistant: What is your email address?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  alpha@gmail.com\n"},{"name":"stdout","text":"\nAssistant: What is your phone number?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  9191919191\n"},{"name":"stdout","text":"\nAssistant: How many years of professional experience do you have?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  0\n"},{"name":"stdout","text":"\nAssistant: What is your desired position?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  data scientist\n"},{"name":"stdout","text":"\nAssistant: What is your current location?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  Pune\n"},{"name":"stdout","text":"\nAssistant: Please list your technical skills (comma-separated):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  C, C++, Python, HTML5, CSS3, SQL, TensorFlow, Numpy, Pandas, Scikit Learn, OpenCV, Pytorch, Langchain, Data Science, Machine Learning, NLP, Deep Learning\n"},{"name":"stdout","text":"\nAssistant: Thank you! Let's begin the technical assessment. You'll be asked 5 questions.\n\nQuestion 1: Explain how you would use NumPy to efficiently compute the mean and standard deviation of a large dataset loaded from a CSV file using pandas, and discuss considerations for handling memory constraints.\n\nIf you need clarification on any question, type 'query: your question'.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  next\n"},{"name":"stdout","text":"\nAssistant: Question 2: Describe the process of training a simple linear regression model using scikit-learn, including data preprocessing steps, feature scaling, and evaluating model performance metrics like R² score.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  next\n"},{"name":"stdout","text":"\nAssistant: Question 3: How would you implement a basic image classification pipeline using TensorFlow/Keras, starting from loading an ImageNet-preprocessed dataset, defining a CNN architecture, compiling the model, and performing cross-validation?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  next\n"},{"name":"stdout","text":"\nAssistant: Question 4: Discuss the differences between supervised and unsupervised learning, providing examples of algorithms used in each category and scenarios where one might be preferred over the other in real-world applications.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  next\n"},{"name":"stdout","text":"\nAssistant: Question 5: What strategies would you employ to optimize the inference speed of a trained PyTorch model deployed in production, considering factors such as GPU utilization, quantization techniques, and模型 pruning methods?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  query: please clarify the question\n"},{"name":"stdout","text":"\nAssistant: Thank you for your query. Here's the revised question:\n\nHow would you approach optimizing the inference latency of a large-scale deep learning model implemented in PyTorch for real-time applications, while balancing computational efficiency and maintaining acceptable prediction accuracy? Specifically, outline your strategy for leveraging GPU acceleration through framework-specific optimizations, applying state-of-the-art quantization techniques (including but not limited to post-training static量化、dynamic量化, and mixed-precision training), and implementing effective model pruning methods (such as magnitude-based pruning, channel pruning, or structural pruning) tailored to different neural network architectures. Additionally, explain how you would evaluate the impact of these optimizations on overall throughput, memory footprint, and energy consumption during deployment.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Candidate:  next\n"},{"name":"stdout","text":"\nAssistant: Thank you for completing the assessment! Our team will review your answers and contact you soon.\n\n===== Conversation Summary =====\nTotal Interactions: 51\n\nSentiment Analysis:\n- Positive Responses: 6\n- Negative Responses: 1\n- Neutral Responses: 0\n- Average Sentiment Score: 0.92\n- Overall Tone: Positive\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}